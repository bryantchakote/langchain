{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Vectorstores, Embeddings, Retrieval\n",
    "\n",
    "The splitted documents (chunks) has to be put into indexes so that they can be easily retrieved when it comes to answer questions based on the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loaders = [\n",
    "    PyPDFLoader(\"docs/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"docs/MachineLearning-Lecture02.pdf\"),\n",
    "    PyPDFLoader(\"docs/MachineLearning-Lecture03.pdf\"),\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"I love dogs\"\n",
    "str2 = \"I love canines\"  # \"closer\" to str1\n",
    "str3 = \"Cameroon is a beautiful country\"  # \"far\" from str1\n",
    "\n",
    "emb1 = embedding.embed_query(str1)\n",
    "emb2 = embedding.embed_query(str2)\n",
    "emb3 = embedding.embed_query(str3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb1.emb2 = 0.9631664400467244\n",
      "emb1.emb3 = 0.7630367804422375\n",
      "emb2.emb3 = 0.7693401811468166\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"emb1.emb2 =\", np.dot(emb1, emb2))  # \"high\"\n",
    "print(\"emb1.emb3 =\", np.dot(emb1, emb3))  # \"low\"\n",
    "print(\"emb2.emb3 =\", np.dot(emb2, emb3))  # \"low\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "persist_directory = \"docs/chroma\"  # to save the index\n",
    "\n",
    "# Remove old database files if any\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the vectorstore\n",
    "# Chroma is an exemple of vectorstore (lightweight and in-memory)\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory,\n",
    ")\n",
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs229-qa@cs.stanford.edu. This goes to an account that's read by all the TAs and me. So \n",
      "rather than sending us email individually, if you send email to this account, it will \n",
      "actually let us get back to you maximally quickly with answers to your questions.  \n",
      "If you're asking questions about homework problems, please say in the subject line which \n",
      "assignment and which question the email refers to, since that will also help us to route \n",
      "your question to the appropriate TA or to me appropriately and get the response back to \n",
      "you quickly.  \n",
      "Let's see. Skipping ahead — let's see — for homework, one midterm, one open and term \n",
      "project. Notice on the honor code. So one thing that I think will help you to succeed and \n",
      "do well in this class and even help you to enjoy this class more is if you form a study \n",
      "group.  \n",
      "So start looking around where you're sitting now or at the end of class today, mingle a \n",
      "little bit and get to know your classmates. I strongly encourage you to form study groups \n",
      "and sort of have a group of people to study with and have a group of your fellow students \n",
      "to talk over these concepts with. You can also post on the class newsgroup if you want to \n",
      "use that to try to form a study group.  \n",
      "But some of the problems sets in this class are reasonably difficult. People that have \n",
      "taken the class before may tell you they were very difficult. And just I bet it would be \n",
      "more fun for you, and you'd probably have a better learning experience if you form a\n"
     ]
    }
   ],
   "source": [
    "question = \"Is there an email I can ask for help?\"\n",
    "docs = vectordb.similarity_search(question, k=3)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he says it in sort of a really touching, sincere way, and then he has this — you can see it \n",
      "in his eyes — he has this deep appreciation of the truth and beauty in the universe as \n",
      "revealed to him by the math he does.  \n",
      "In this class, I'm not gonna do any truth and beauty. In this class, I'm gonna talk about \n",
      "learning theory to try to convey to you an understanding of how and why learning \n",
      "algorithms work so that we can apply these learning algorithms as effectively as possible.  \n",
      "So, for example, it turns out you can prove surprisingly deep theorems on when you can \n",
      "guarantee that a learning algorithm will work, all right? So think about a learning\n"
     ]
    }
   ],
   "source": [
    "# Even if the question has nothing to do with the documents,\n",
    "# the similarity search will return the top k based on a metric\n",
    "question = \"What is said about Sergio Ramos?\"\n",
    "docs = vectordb.similarity_search(question, k=3)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(metadata={}, page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Talks about a species of mushroom (Amanita phalloides)\n",
    "texts = [\n",
    "    \"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\",\n",
    "    \"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\",\n",
    "    \"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\",\n",
    "]\n",
    "\n",
    "vectordb = Chroma.from_texts(texts, embedding=embedding)\n",
    "question = \"Tell me about all-white mushrooms with large fruiting bodies\"\n",
    "vectordb.similarity_search(question, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(metadata={}, page_content='A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max Marginal Relevance (MMR):\n",
    "# - select the top k1 documents\n",
    "# - among them select the k2 most diverse\n",
    "vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-query: the answer to the query is restricted to certain documents\n",
    "# There's a need to filter on metadata, which is not the natural behaviour\n",
    "# of similarity_search; the latter only compares the embeddings of the content\n",
    "# We use an LLM to guide the retriever if needed\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo  # to describe the metadata\n",
    "\n",
    "# Retrieve the previously saved index\n",
    "vectordb = Chroma(\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=persist_directory,\n",
    ")\n",
    "\n",
    "\n",
    "# Metadata description\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The lecture the chunk is from, should be one of `docs/MachineLearning-Lecture01.pdf`, `docs/MachineLearning-Lecture02.pdf`, or `docs/MachineLearning-Lecture03.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "llm = OpenAI()\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    vectorstore=vectordb,\n",
    "    document_contents=\"Machine Learning lecture notes\",  # document content description\n",
    "    metadata_field_info=metadata_field_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 0, 'source': 'docs/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 10, 'source': 'docs/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 5, 'source': 'docs/MachineLearning-Lecture03.pdf'}\n",
      "{'page': 2, 'source': 'docs/MachineLearning-Lecture03.pdf'}\n"
     ]
    }
   ],
   "source": [
    "question = \"What did they say about regression in the third lecture?\"\n",
    "docs = retriever.invoke({\"input\": question})\n",
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "- MATLAB or in Octave\n",
      "- easy to learn tool to use for implementing a lot of learning algorithms\n",
      "- Octave\n",
      "- a software package called Octave\n",
      "- fewer features than MATLAB\n",
      "- free\n",
      "- just about everything\n",
      "- once a colleague of mine at a different university, not at Stanford, actually teaches another machine learning course.\n",
      "---\n",
      "Document 2:\n",
      "\"learning theory to try to convey to you an understanding of how and why learning algorithms work so that we can apply these learning algorithms as effectively as possible.\"\n",
      "---\n",
      "Document 3:\n",
      "\"Instructor (Andrew Ng):Say that again.\"\n"
     ]
    }
   ],
   "source": [
    "# Compression: pass only the relevant sentences of the relevant documents to the LLM\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n---\\n\".join([f\"Document {i+1}:\\n\" + doc.page_content for i, doc in enumerate(docs)]))\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "retriever = vectordb.as_retriever(search_type=\"mmr\")  # combine techniques\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "question = \"what did they say about matlab?\"\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
