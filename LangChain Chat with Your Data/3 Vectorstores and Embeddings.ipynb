{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Vectorstores, Embeddings, Retrieval\n",
    "\n",
    "The splitted documents (chunks) has to be put into indexes so that they can be easily retrieved when it comes to answer questions based on the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loaders = [\n",
    "    PyPDFLoader(\"docs/MachineLearning-Lecture01.pdf\"),\n",
    "    PyPDFLoader(\"docs/MachineLearning-Lecture02.pdf\"),\n",
    "    PyPDFLoader(\"docs/MachineLearning-Lecture03.pdf\"),\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"I love dogs\"\n",
    "str2 = \"I love canines\"  # \"closer\" to str1\n",
    "str3 = \"Cameroon is a beautiful country\"  # \"far\" from str1\n",
    "\n",
    "emb1 = embedding.embed_query(str1)\n",
    "emb2 = embedding.embed_query(str2)\n",
    "emb3 = embedding.embed_query(str3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb1.emb2 = 0.9631664400467244\n",
      "emb1.emb3 = 0.7630367804422375\n",
      "emb2.emb3 = 0.7693401811468166\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"emb1.emb2 =\", np.dot(emb1, emb2))  # \"high\"\n",
    "print(\"emb1.emb3 =\", np.dot(emb1, emb3))  # \"low\"\n",
    "print(\"emb2.emb3 =\", np.dot(emb2, emb3))  # \"low\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: 'docs/chroma\\\\0e831259-a9b4-4d8e-a22d-c480d44ec109\\\\data_level0.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Remove old database files if any\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(persist_directory):\n\u001b[1;32m----> 8\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(persist_directory)\n",
      "File \u001b[1;32mc:\\Users\\tchak\\anaconda3\\envs\\langchain\\Lib\\shutil.py:781\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror, onexc, dir_fd)\u001b[0m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;66;03m# can't continue even if onexc hook returns\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 781\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _rmtree_unsafe(path, onexc)\n",
      "File \u001b[1;32mc:\\Users\\tchak\\anaconda3\\envs\\langchain\\Lib\\shutil.py:635\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onexc)\u001b[0m\n\u001b[0;32m    633\u001b[0m             os\u001b[38;5;241m.\u001b[39munlink(fullname)\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 635\u001b[0m             onexc(os\u001b[38;5;241m.\u001b[39munlink, fullname, err)\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    637\u001b[0m     os\u001b[38;5;241m.\u001b[39mrmdir(path)\n",
      "File \u001b[1;32mc:\\Users\\tchak\\anaconda3\\envs\\langchain\\Lib\\shutil.py:633\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onexc)\u001b[0m\n\u001b[0;32m    631\u001b[0m fullname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirpath, name)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 633\u001b[0m     os\u001b[38;5;241m.\u001b[39munlink(fullname)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    635\u001b[0m     onexc(os\u001b[38;5;241m.\u001b[39munlink, fullname, err)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: 'docs/chroma\\\\0e831259-a9b4-4d8e-a22d-c480d44ec109\\\\data_level0.bin'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "persist_directory = \"docs/chroma\"  # to save the index\n",
    "\n",
    "# Remove old database files if any\n",
    "if os.path.exists(persist_directory):\n",
    "    shutil.rmtree(persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the vectorstore\n",
    "# Chroma is an exemple of vectorstore (lightweight and in-memory)\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory,\n",
    ")\n",
    "vectordb._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs229-qa@cs.stanford.edu. This goes to an account that's read by all the TAs and me. So \n",
      "rather than sending us email individually, if you send email to this account, it will \n",
      "actually let us get back to you maximally quickly with answers to your questions.  \n",
      "If you're asking questions about homework problems, please say in the subject line which \n",
      "assignment and which question the email refers to, since that will also help us to route \n",
      "your question to the appropriate TA or to me appropriately and get the response back to \n",
      "you quickly.  \n",
      "Let's see. Skipping ahead — let's see — for homework, one midterm, one open and term \n",
      "project. Notice on the honor code. So one thing that I think will help you to succeed and \n",
      "do well in this class and even help you to enjoy this class more is if you form a study \n",
      "group.  \n",
      "So start looking around where you're sitting now or at the end of class today, mingle a \n",
      "little bit and get to know your classmates. I strongly encourage you to form study groups \n",
      "and sort of have a group of people to study with and have a group of your fellow students \n",
      "to talk over these concepts with. You can also post on the class newsgroup if you want to \n",
      "use that to try to form a study group.  \n",
      "But some of the problems sets in this class are reasonably difficult. People that have \n",
      "taken the class before may tell you they were very difficult. And just I bet it would be \n",
      "more fun for you, and you'd probably have a better learning experience if you form a\n"
     ]
    }
   ],
   "source": [
    "question = \"Is there an email I can ask for help?\"\n",
    "docs = vectordb.similarity_search(question, k=3)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he says it in sort of a really touching, sincere way, and then he has this — you can see it \n",
      "in his eyes — he has this deep appreciation of the truth and beauty in the universe as \n",
      "revealed to him by the math he does.  \n",
      "In this class, I'm not gonna do any truth and beauty. In this class, I'm gonna talk about \n",
      "learning theory to try to convey to you an understanding of how and why learning \n",
      "algorithms work so that we can apply these learning algorithms as effectively as possible.  \n",
      "So, for example, it turns out you can prove surprisingly deep theorems on when you can \n",
      "guarantee that a learning algorithm will work, all right? So think about a learning\n"
     ]
    }
   ],
   "source": [
    "# Even if the question has nothing to do with the documents,\n",
    "# the similarity search will return the top k based on a metric\n",
    "question = \"What is said about Sergio Ramos?\"\n",
    "docs = vectordb.similarity_search(question, k=3)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(metadata={}, page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Talks about a species of mushroom (Amanita phalloides)\n",
    "texts = [\n",
    "    \"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\",\n",
    "    \"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\",\n",
    "    \"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\",\n",
    "]\n",
    "\n",
    "vectordb = Chroma.from_texts(texts, embedding=embedding)\n",
    "question = \"Tell me about all-white mushrooms with large fruiting bodies\"\n",
    "vectordb.similarity_search(question, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(metadata={}, page_content='A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max Marginal Relevance (MMR):\n",
    "# - select the top k1 documents\n",
    "# - among them select the k2 most diverse\n",
    "vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-query: the answer to the query is restricted to certain documents\n",
    "# There's a need to filter on metadata, which is not the natural behaviour\n",
    "# of similarity_search; the latter only compares the embeddings of the content\n",
    "# We use an LLM to guide the retriever if needed\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo  # to describe the metadata\n",
    "\n",
    "# Retrieve the previously saved index\n",
    "vectordb = Chroma(\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=persist_directory,\n",
    ")\n",
    "\n",
    "\n",
    "# Metadata description\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The lecture the chunk is from, should be one of `docs/MachineLearning-Lecture01.pdf`, `docs/MachineLearning-Lecture02.pdf`, or `docs/MachineLearning-Lecture03.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "llm = OpenAI()\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    vectorstore=vectordb,\n",
    "    document_contents=\"Machine Learning lecture notes\",  # document content description\n",
    "    metadata_field_info=metadata_field_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What did they say about regression in the third lecture?\"\n",
    "docs = retriever.invoke({\"input\": question})\n",
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "- those homeworks will be done in either MATLAB or in Octave\n",
      "- MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data\n",
      "- there's also — [inaudible] write that down [inaudible] MATLAB — there' s also a software package called Octave that you can download for free off the Internet\n",
      "- it has somewhat fewer features than MATLAB, but it's free, and for the purposes of this class, it will work for just about everything\n",
      "---\n",
      "Document 2:\n",
      "\"In this class, I'm not gonna do any truth and beauty.\"\n",
      "---\n",
      "Document 3:\n",
      "ICA or independent component analysis algorithms\n"
     ]
    }
   ],
   "source": [
    "# Compression: pass only the relevant sentences of the relevant documents to the LLM\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n---\\n\".join([f\"Document {i+1}:\\n\" + doc.page_content for i, doc in enumerate(docs)]))\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "retriever = vectordb.as_retriever(search_type=\"mmr\")  # combine techniques\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "question = \"what did they say about matlab?\"\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
