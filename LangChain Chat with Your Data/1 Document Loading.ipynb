{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"docs/MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata:\n",
      " {'source': 'docs/MachineLearning-Lecture01.pdf', 'page': 0}\n",
      "\n",
      "Content:\n",
      " MachineLearning-Lecture01  \n",
      "Instructor (Andrew Ng): Okay. Good morning. Welcome to CS229, the machine \n",
      "learning class. So what I wanna do today is just spend a little time going over the logistics \n",
      "of the class, and then we'll start to talk a bit about machine learning.  \n",
      "By way of introduction, my name's Andrew Ng and I'll be instructor for this class. And so \n",
      "I personally work in machine learning, and I've worked on it for about 15 years now, and \n",
      "I actually think that machine learning is the \n"
     ]
    }
   ],
   "source": [
    "page = pages[0]\n",
    "print(\"Metadata:\\n\", page.metadata)\n",
    "print()\n",
    "print(\"Content:\\n\", page.page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=XUFLq6dKQok\n",
      "[youtube] XUFLq6dKQok: Downloading webpage\n",
      "[youtube] XUFLq6dKQok: Downloading ios player API JSON\n",
      "[youtube] XUFLq6dKQok: Downloading mweb player API JSON\n",
      "[youtube] XUFLq6dKQok: Downloading m3u8 information\n",
      "[info] XUFLq6dKQok: Downloading 1 format(s): 140\n",
      "[download] docs\\youtube\\FORMATION DEEP LEARNING COMPLETE (2021).m4a has already been downloaded\n",
      "[download] 100% of   28.63MiB\n",
      "[ExtractAudio] Not converting audio docs\\youtube\\FORMATION DEEP LEARNING COMPLETE (2021).m4a; file is already in target format m4a\n",
      "Transcribing part 1!\n",
      "Transcribing part 2!\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
    "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=XUFLq6dKQok\"\n",
    "save_dir = \"docs/youtube\"\n",
    "loader = GenericLoader(\n",
    "    YoutubeAudioLoader([url], save_dir),\n",
    "    OpenAIWhisperParser(),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ceci est un r√©seau de neurones artificiels, un des algorithmes '\n",
      " \"d'intelligence artificielle les plus sophistiqu√©s au monde. A l'origine \"\n",
      " 'inspir√©e du fonctionnement des neurones biologiques, cet algorithme est '\n",
      " \"capable d'apprendre √† r√©aliser n'importe quelle t√¢che. Conduire une voiture, \"\n",
      " 'jouer aux √©checs, entretenir une conversation, ou encore reconna√Ætre et '\n",
      " 'classer des images telles que ces chiffres que vous voyez en ce moment √† '\n",
      " \"l'√©cran. Dans cette s√©rie de vid√©os, je vais vous montrer comment cr√©er\")\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(docs[0].page_content[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://github.com/bryantchakote/bryantchakote/blob/main/README.md\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata:\n",
      "{\n",
      "    \"source\": \"https://github.com/bryantchakote/bryantchakote/blob/main/README.md\",\n",
      "    \"title\": \"bryantchakote/README.md at main \\u00b7 bryantchakote/bryantchakote \\u00b7 GitHub\",\n",
      "    \"description\": \"Contribute to bryantchakote/bryantchakote development by creating an account on GitHub.\",\n",
      "    \"language\": \"en\"\n",
      "}\n",
      "\n",
      "Content:\n",
      "Hello, I'm Bryan Tchakote üòÅ\n",
      "A Student with a deep passion for Data Science & AI\n",
      "üèãüèæ‚Äç‚ôÇÔ∏è So...\n",
      "ü´∞üèæI‚Äôm currently learning a bit of everything ü§∑üèæ‚Äç‚ôÇÔ∏è\n",
      "üêíAsk me about Machine & Deep Learning models, Computer Vision, and whatever else...\n",
      "üåúFun Fact: (Just because I saw it somewhere)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "page = docs[0]\n",
    "metadata = page.metadata\n",
    "page_content = page.page_content\n",
    "page_content = re.sub(r\"\\s{2,}\", \"\\n\", page_content)  # remove trailing spaces\n",
    "\n",
    "print(\"Metadata:\\n\" + json.dumps(metadata, indent=4))\n",
    "print()\n",
    "print(\"Content:\\n\" + page_content[-504:-233])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "loader = NotionDirectoryLoader(\"docs/notion\")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata:\n",
      " {'source': 'docs\\\\notion\\\\Paper 7 LayoutLMv3 Pre-training for Document AI wi d273f2f8a44245479c115d71f0b1cddb.md'}\n",
      "\n",
      "Content:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Paper 7: LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking\n",
       "\n",
       "---\n",
       "\n",
       "Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. In Proceedings of the 30th ACM International Conference on Multimedia(MM‚Äô22), October 10‚Äì14, 2022, Lisboa, Portugal. ACM, New York, NY, USA, 10 pages. [https://doi.org/10.1145/3503161.3548112](https://doi.org/10.1145/3503161.3548112)\n",
       "\n",
       "---\n",
       "\n",
       "# Abstract\n",
       "\n",
       "- Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified text and image masking.\n",
       "- Experimental results show that LayoutLMv3 achieves state of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual\n",
       "question answering, but also in image-centric tasks such as document image classification and document layout analysis.\n",
       "\n",
       "# Introduction\n",
       "\n",
       "- A pre-trained Document AI model can parse layout and extract key information for various documents such as scanned forms and academic papers.\n",
       "- Comparisons with existing works (e.g., DocFormer [2] and SelfDoc [31])\n",
       "    - On image embedding: our LayoutLMv3 uses linear patches to reduce the computational bottleneck of CNNs and eliminate the need for region supervision in training object detectors.\n",
       "    - On pre-training objectives on image modality: our LayoutLMv3 learns to reconstruct discrete image tokens of masked patches instead of raw pixels or region features to capture high-level layout structures rather than noisy details.\n",
       "- To overcome the discrepancy in pre-training objectives of text and image modalities and facilitate multimodal representation learning, we propose LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified text and image masking objectives MLM (Masked Language Modeling) and MIM (Masked Image Modeling).\n",
       "- Inspired by DALL-E [43] and BEiT [3], we obtain the target image tokens from latent codes of a discrete VAE. For documents, each text word corresponds to an image patch. To learn this cross-modal alignment, we propose a Word-Patch Alignment (WPA) objective to predict whether the corresponding image patch of a text word is masked.\n",
       "- Inspired by ViT [11] and ViLT [22], LayoutLMv3 directly leverages raw image patches from document images without complex pre-processing steps such as page object detection.\n",
       "- LayoutLMv3 jointly learns image, text and multimodal representations in a Transformer model with unified MLM, MIM and WPA objectives. This makes LayoutLMv3 the first multimodal pre-trained Document AI model without CNNs for image embeddings, which significantly saves parameters and gets rid of region annotations.\n",
       "- The simple unified architecture and objectives make LayoutLMv3 a general purpose pre-trained model for both text-centric tasks and image centric Document AI tasks.\n",
       "\n",
       "# LayoutLMv3\n",
       "\n",
       "## Model Architecture\n",
       "\n",
       "- LayoutLMv3 applies a unified text-image multimodal Transformer to learn cross-modal representations. The Transformer has a multi-layer architecture and each layer mainly consists of multi-head self-attention and position-wise fully connected feed-forward networks [49]. The input of Transformer is a concatenation of text embedding Y = y1:ùêø and image embedding X = x1:ùëÄ sequences, where ùêø and ùëÄ are sequence lengths for text and image respectively. Through the Transformer, the last layer outputs text-and-image contextual representations.\n",
       "- Text embedding is a combination of word embeddings and position embeddings. We pre-processed document images with an off-the-shelf OCR toolkit to obtain textual content and corresponding 2D position information. We initialize the word embeddings with a word embedding matrix from a pre-trained model RoBERTa. The position embeddings include 1D position and 2D layout position embeddings, where the 1D position refers to the index of tokens within the text sequence, and the 2D layout position refers to the bounding box coordinates of the text sequence.\n",
       "- Image Embedding: Inspired by ViT [11] and ViLT [22], we represent document images with linear projection features of image patches before feeding them in to the multimodal Transformer. [‚Ä¶] We insert semantic 1D relative position and spatial 2D relative position as bias terms in self-attention networks for text and image modalities following LayoutLMv2[56].\n",
       "\n",
       "## Pre-training Objectives\n",
       "\n",
       "Objective 1: Masked Language Modeling (MLM). We mask 30% of text tokens with a span masking strategy with span lengths drawn from a Poisson distribution (ùúÜ = 3) [21, 27].\n",
       "\n",
       "Objective 2: Masked Image Modeling (MIM). The MIM objective is a symmetry to the MLM objective, that we randomly mask a percentage of about 40% image tokens with the blockwise masking strategy [3].\n",
       "\n",
       "Objective 3: Word-Patch Alignment (WPA). For documents, each text word corresponds to an image patch. As we randomly mask text and image tokens with MLM and MIM respectively, there is no explicit alignment learning between text and image modalities. We thus propose a WPA objective to learn a fine-grained alignment between text words and image patches. The WPA objective is to predict whether the corresponding image patches of a text word are masked.\n",
       "\n",
       "# Experiments\n",
       "\n",
       "## Fine-tuning on Multimodal Tasks\n",
       "\n",
       "- Task 1: Form and Receipt Understanding. Form and receipt understanding tasks require extracting and structuring forms and receipts‚Äô textual content. The tasks are a sequence labeling problem aiming to tag each word with a label. We predict the label of the last hidden state of each text token with a linear layer and an MLP classifier for form and receipt understanding tasks, respectively.\n",
       "- Task 2: Document Image Classification. The document image classification task aims to predict the category of document images. We feed the output hidden state of the special classification token ([CLS]) into an MLP classifier to predict the class labels."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "page = docs[9]\n",
    "print(\"Metadata:\\n\", page.metadata)\n",
    "print()\n",
    "print(\"Content:\\n\")\n",
    "display(Markdown(page.page_content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
